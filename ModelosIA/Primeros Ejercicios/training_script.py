"""
SCRIPT DE ENTRENAMIENTO DEL MODELO HÃBRIDO - VERSIÃ“N COMPLETA
Entrena la red neuronal con datos sintÃ©ticos y evalÃºa el sistema completo
"""

import json
import numpy as np
import os
from datetime import datetime
from typing import List, Dict, Tuple

# Verificar e importar dependencias
print("ğŸ”„ Verificando dependencias...")

# Verificar matplotlib
try:
    import matplotlib.pyplot as plt

    matplotlib_available = True
    print("âœ… Matplotlib disponible")
except ImportError:
    matplotlib_available = False
    print("âš ï¸ Matplotlib no disponible - las grÃ¡ficas no funcionarÃ¡n")
    print("ğŸ’¡ Instala con: pip install matplotlib")

# Verificar TensorFlow
try:
    import tensorflow as tf

    print(f"âœ… TensorFlow {tf.__version__} disponible")
    tf.get_logger().setLevel('ERROR')
except ImportError:
    print("âŒ Error: TensorFlow no estÃ¡ instalado")
    print("ğŸ’¡ Instala con: pip install tensorflow")
    exit(1)

# Importar nuestros mÃ³dulos
try:
    from hybrid_model import HybridMathModel
    from training_data_generator import TrainingDataGenerator

    print("âœ… MÃ³dulos del proyecto importados correctamente")
except ImportError as e:
    print(f"âŒ Error importando mÃ³dulos: {e}")
    print("ğŸ’¡ AsegÃºrate de tener los archivos:")
    print("   - hybrid_model.py")
    print("   - training_data_generator.py")
    print("   - training_script.py (este archivo)")
    exit(1)


class ModelTrainer:
    """
    Entrenador del modelo hÃ­brido con evaluaciÃ³n completa
    """

    def __init__(self):
        self.model = HybridMathModel()
        self.data_generator = TrainingDataGenerator()
        self.training_history = {}
        self.evaluation_results = {}

    def prepare_training_data(self, dataset: List[Dict]) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Prepara los datos del dataset para el entrenamiento de la red neuronal
        """
        print("ğŸ”„ Preparando datos para entrenamiento...")

        X_tema = []
        X_dificultad = []
        X_usuario = []
        y_tokens = []

        # TamaÃ±o del vocabulario (debe coincidir con el modelo)
        vocab_size = self.model.neural_generator.vocab_size

        for exercise in dataset:
            metadata = exercise.get('training_metadata', {})

            # Entradas
            X_tema.append(metadata.get('topic_encoded', 0))
            X_dificultad.append(metadata.get('difficulty_encoded', 0))

            # Perfil de usuario
            user_profile = metadata.get('user_profile_sample', {})
            edad_norm = (user_profile.get('edad', 12) - 8) / 8  # Normalizar edad
            rendimiento = user_profile.get('rendimiento', 0.5)
            preferencia = user_profile.get('preferencia_visual', 0.5)
            X_usuario.append([edad_norm, rendimiento, preferencia])

            # Target (tokens de salida)
            target_tokens = metadata.get('target_tokens', [0])
            # Crear vector one-hot para el primer token (simplificado)
            target_vector = np.zeros(vocab_size)
            if target_tokens and len(target_tokens) > 0:
                first_token = target_tokens[0]
                if first_token < vocab_size:
                    target_vector[first_token] = 1.0
                else:
                    target_vector[0] = 1.0  # Padding token
            else:
                target_vector[0] = 1.0

            y_tokens.append(target_vector)

        # Convertir a arrays numpy
        X_tema = np.array(X_tema).reshape(-1, 1)
        X_dificultad = np.array(X_dificultad).reshape(-1, 1)
        X_usuario = np.array(X_usuario)
        y_tokens = np.array(y_tokens)

        print(f"âœ… Datos preparados:")
        print(f"   Muestras: {len(X_tema)}")
        print(f"   Forma X_tema: {X_tema.shape}")
        print(f"   Forma X_dificultad: {X_dificultad.shape}")
        print(f"   Forma X_usuario: {X_usuario.shape}")
        print(f"   Forma y_tokens: {y_tokens.shape}")

        return X_tema, X_dificultad, X_usuario, y_tokens

    def train_model(self, dataset_file: str = None, epochs: int = 50, batch_size: int = 32) -> Dict:
        """
        Entrena el modelo completo
        """

        print("ğŸ“ INICIANDO ENTRENAMIENTO DEL MODELO HÃBRIDO")
        print("=" * 60)

        # Paso 1: Obtener dataset
        if dataset_file and os.path.exists(dataset_file):
            print(f"ğŸ“‚ Cargando dataset existente: {dataset_file}")
            dataset, metadata = self.data_generator.load_dataset(dataset_file)
        else:
            print("ğŸ“Š Generando nuevo dataset de entrenamiento...")
            dataset = self.data_generator.generate_training_dataset(samples_per_topic=200)
            dataset_file = self.data_generator.save_dataset(dataset)

        if not dataset:
            print("âŒ Error: No se pudo obtener dataset para entrenamiento")
            return {}

        # Paso 2: Preparar datos
        X_tema, X_dificultad, X_usuario, y_tokens = self.prepare_training_data(dataset)

        # Paso 3: Construir modelo si no existe
        if self.model.neural_generator.model is None:
            print("ğŸ—ï¸ Construyendo arquitectura de red neuronal...")
            self.model.neural_generator.build_model()
            print("âœ… Modelo construido!")

            # Mostrar resumen del modelo
            try:
                print("\nğŸ“‹ Resumen del modelo:")
                self.model.neural_generator.model.summary()
            except:
                print("   (Resumen no disponible)")

        # Paso 4: Entrenar red neuronal
        print(f"\nğŸ§  Entrenando red neuronal...")
        print(f"   Ã‰pocas: {epochs}")
        print(f"   Batch size: {batch_size}")
        print(f"   Muestras de entrenamiento: {len(X_tema)}")

        try:
            history = self.model.neural_generator.model.fit(
                [X_tema, X_dificultad, X_usuario],
                y_tokens,
                epochs=epochs,
                batch_size=batch_size,
                validation_split=0.2,
                verbose=1,
                shuffle=True
            )

            # Marcar como entrenado
            self.model.neural_generator.is_trained = True

            # Guardar historial
            self.training_history = {
                'loss': history.history['loss'],
                'accuracy': history.history.get('accuracy', []),
                'val_loss': history.history.get('val_loss', []),
                'val_accuracy': history.history.get('val_accuracy', []),
                'epochs': epochs,
                'batch_size': batch_size,
                'total_samples': len(X_tema),
                'dataset_file': dataset_file,
                'trained_at': datetime.now().isoformat()
            }

            print("âœ… Entrenamiento completado!")

            # Paso 5: Evaluar modelo
            self.evaluate_model(dataset[:100])  # Evaluar con subset

            return self.training_history

        except Exception as e:
            print(f"âŒ Error durante entrenamiento: {e}")
            return {}

    def evaluate_model(self, test_dataset: List[Dict]) -> Dict:
        """
        EvalÃºa el modelo hÃ­brido entrenado
        """
        print("\nğŸ” EVALUANDO MODELO ENTRENADO")
        print("-" * 40)

        evaluation_results = {
            'total_tests': len(test_dataset),
            'successful_generations': 0,
            'validation_success_rate': 0.0,
            'average_confidence': 0.0,
            'average_attempts': 0.0,
            'topic_performance': {},
            'difficulty_performance': {},
            'errors': []
        }

        confidences = []
        attempts_list = []

        # Probar generaciÃ³n con diferentes parÃ¡metros
        test_scenarios = [
            ('aritmetica', 'basico'),
            ('aritmetica', 'intermedio'),
            ('algebra', 'basico'),
            ('algebra', 'intermedio'),
            ('geometria', 'basico'),
            ('fracciones', 'basico')
        ]

        print("Probando generaciÃ³n de ejercicios...")

        for i, (topic, difficulty) in enumerate(test_scenarios):
            print(f"  {i + 1}. {topic} - {difficulty}...", end="")

            try:
                # Perfil de usuario de prueba
                user_profile = {
                    'edad': 12,
                    'rendimiento': 0.7,
                    'preferencia_visual': 0.5
                }

                # Generar ejercicio
                exercise = self.model.generate_validated_exercise(
                    topic, difficulty, user_profile, max_attempts=3
                )

                # Analizar resultado
                validation = exercise.get('validation', {})
                is_valid = validation.get('is_valid', False)
                confidence = validation.get('confidence', 0.0)
                attempts = validation.get('attempts_needed', 0)

                if is_valid:
                    evaluation_results['successful_generations'] += 1
                    confidences.append(confidence)

                    if isinstance(attempts, int):
                        attempts_list.append(attempts)

                    print(" âœ…")
                else:
                    print(" âŒ")
                    evaluation_results['errors'].append(f"{topic}-{difficulty}: {validation.get('errors', [])}")

                # EstadÃ­sticas por tema
                if topic not in evaluation_results['topic_performance']:
                    evaluation_results['topic_performance'][topic] = {'success': 0, 'total': 0}

                evaluation_results['topic_performance'][topic]['total'] += 1
                if is_valid:
                    evaluation_results['topic_performance'][topic]['success'] += 1

                # EstadÃ­sticas por dificultad
                if difficulty not in evaluation_results['difficulty_performance']:
                    evaluation_results['difficulty_performance'][difficulty] = {'success': 0, 'total': 0}

                evaluation_results['difficulty_performance'][difficulty]['total'] += 1
                if is_valid:
                    evaluation_results['difficulty_performance'][difficulty]['success'] += 1

            except Exception as e:
                print(f" âŒ Error: {e}")
                evaluation_results['errors'].append(f"{topic}-{difficulty}: {str(e)}")

        # Calcular mÃ©tricas finales
        total_scenarios = len(test_scenarios)
        evaluation_results['validation_success_rate'] = (
                evaluation_results['successful_generations'] / total_scenarios
        ) if total_scenarios > 0 else 0.0

        evaluation_results['average_confidence'] = (
                sum(confidences) / len(confidences)
        ) if confidences else 0.0

        evaluation_results['average_attempts'] = (
                sum(attempts_list) / len(attempts_list)
        ) if attempts_list else 0.0

        # Mostrar resultados
        print(f"\nğŸ“Š RESULTADOS DE EVALUACIÃ“N:")
        print(f"   âœ… Generaciones exitosas: {evaluation_results['successful_generations']}/{total_scenarios}")
        print(f"   ğŸ“ˆ Tasa de Ã©xito: {evaluation_results['validation_success_rate']:.2%}")
        print(f"   ğŸ¯ Confianza promedio: {evaluation_results['average_confidence']:.2f}")
        print(f"   ğŸ”„ Intentos promedio: {evaluation_results['average_attempts']:.1f}")

        print(f"\nğŸ“š Rendimiento por tema:")
        for topic, stats in evaluation_results['topic_performance'].items():
            success_rate = stats['success'] / stats['total'] if stats['total'] > 0 else 0
            print(f"   {topic}: {stats['success']}/{stats['total']} ({success_rate:.1%})")

        print(f"\nğŸšï¸ Rendimiento por dificultad:")
        for difficulty, stats in evaluation_results['difficulty_performance'].items():
            success_rate = stats['success'] / stats['total'] if stats['total'] > 0 else 0
            print(f"   {difficulty}: {stats['success']}/{stats['total']} ({success_rate:.1%})")

        if evaluation_results['errors']:
            print(f"\nâš ï¸ Errores encontrados:")
            for error in evaluation_results['errors'][:3]:  # Mostrar solo primeros 3
                print(f"   {error}")

        self.evaluation_results = evaluation_results
        return evaluation_results

    def plot_training_history(self, save_plot: bool = True):
        """
        Grafica el historial de entrenamiento
        """
        if not matplotlib_available:
            print("âŒ Matplotlib no estÃ¡ disponible para crear grÃ¡ficas")
            return

        if not self.training_history:
            print("âŒ No hay historial de entrenamiento para graficar")
            return

        try:
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))

            epochs = range(1, len(self.training_history['loss']) + 1)

            # Loss
            ax1.plot(epochs, self.training_history['loss'], 'b-', label='Training Loss')
            if 'val_loss' in self.training_history and self.training_history['val_loss']:
                ax1.plot(epochs, self.training_history['val_loss'], 'r-', label='Validation Loss')
            ax1.set_title('Model Loss')
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('Loss')
            ax1.legend()
            ax1.grid(True)

            # Accuracy
            if 'accuracy' in self.training_history and self.training_history['accuracy']:
                ax2.plot(epochs, self.training_history['accuracy'], 'b-', label='Training Accuracy')
                if 'val_accuracy' in self.training_history and self.training_history['val_accuracy']:
                    ax2.plot(epochs, self.training_history['val_accuracy'], 'r-', label='Validation Accuracy')
                ax2.set_title('Model Accuracy')
                ax2.set_xlabel('Epoch')
                ax2.set_ylabel('Accuracy')
                ax2.legend()
                ax2.grid(True)
            else:
                ax2.text(0.5, 0.5, 'No accuracy data available',
                         horizontalalignment='center', verticalalignment='center',
                         transform=ax2.transAxes)
                ax2.set_title('Model Accuracy (No Data)')

            # Evaluation Results
            if self.evaluation_results:
                topics = list(self.evaluation_results['topic_performance'].keys())
                success_rates = [
                    self.evaluation_results['topic_performance'][topic]['success'] /
                    self.evaluation_results['topic_performance'][topic]['total']
                    for topic in topics
                ]

                ax3.bar(topics, success_rates, color='green', alpha=0.7)
                ax3.set_title('Success Rate by Topic')
                ax3.set_ylabel('Success Rate')
                ax3.set_ylim(0, 1)
                plt.setp(ax3.get_xticklabels(), rotation=45)

                # Difficulty performance
                difficulties = list(self.evaluation_results['difficulty_performance'].keys())
                diff_success_rates = [
                    self.evaluation_results['difficulty_performance'][diff]['success'] /
                    self.evaluation_results['difficulty_performance'][diff]['total']
                    for diff in difficulties
                ]

                ax4.bar(difficulties, diff_success_rates, color='blue', alpha=0.7)
                ax4.set_title('Success Rate by Difficulty')
                ax4.set_ylabel('Success Rate')
                ax4.set_ylim(0, 1)
            else:
                ax3.text(0.5, 0.5, 'No evaluation data',
                         horizontalalignment='center', verticalalignment='center',
                         transform=ax3.transAxes)
                ax4.text(0.5, 0.5, 'No evaluation data',
                         horizontalalignment='center', verticalalignment='center',
                         transform=ax4.transAxes)

            plt.tight_layout()

            if save_plot:
                plot_filename = f"training_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
                print(f"ğŸ“Š GrÃ¡fica guardada: {plot_filename}")

            plt.show()

        except Exception as e:
            print(f"âŒ Error creando grÃ¡fica: {e}")

    def save_trained_model(self, model_filename: str = None):
        """
        Guarda el modelo entrenado con todos los metadatos
        """
        if model_filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_filename = f"trained_hybrid_model_{timestamp}.json"

        # Guardar modelo
        self.model.save_model(model_filename)

        # Guardar historial de entrenamiento y evaluaciÃ³n
        training_data = {
            'training_history': self.training_history,
            'evaluation_results': self.evaluation_results,
            'model_stats': self.model.get_model_stats(),
            'saved_at': datetime.now().isoformat()
        }

        training_filename = model_filename.replace('.json', '_training_data.json')
        with open(training_filename, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ Modelo completo guardado:")
        print(f"   ğŸ“‹ Modelo: {model_filename}")
        print(f"   ğŸ“Š Datos de entrenamiento: {training_filename}")

        return model_filename, training_filename

    def generate_training_report(self) -> str:
        """
        Genera un reporte completo del entrenamiento
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"training_report_{timestamp}.md"

        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write("# ğŸ“ REPORTE DE ENTRENAMIENTO - MODELO HÃBRIDO MATH_M1M\n\n")
            f.write(f"**Fecha:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            # InformaciÃ³n del entrenamiento
            if self.training_history:
                f.write("## ğŸ“Š InformaciÃ³n del Entrenamiento\n\n")
                f.write(f"- **Ã‰pocas:** {self.training_history.get('epochs', 'N/A')}\n")
                f.write(f"- **Batch Size:** {self.training_history.get('batch_size', 'N/A')}\n")
                f.write(f"- **Muestras totales:** {self.training_history.get('total_samples', 'N/A')}\n")
                f.write(f"- **Dataset:** {self.training_history.get('dataset_file', 'N/A')}\n\n")

                # MÃ©tricas finales
                if self.training_history.get('loss'):
                    final_loss = self.training_history['loss'][-1]
                    f.write(f"- **Loss final:** {final_loss:.4f}\n")

                if self.training_history.get('val_loss'):
                    final_val_loss = self.training_history['val_loss'][-1]
                    f.write(f"- **Validation Loss final:** {final_val_loss:.4f}\n")

                f.write("\n")

            # Resultados de evaluaciÃ³n
            if self.evaluation_results:
                f.write("## ğŸ” Resultados de EvaluaciÃ³n\n\n")
                f.write(f"- **Tasa de Ã©xito:** {self.evaluation_results['validation_success_rate']:.2%}\n")
                f.write(f"- **Confianza promedio:** {self.evaluation_results['average_confidence']:.2f}\n")
                f.write(f"- **Intentos promedio:** {self.evaluation_results['average_attempts']:.1f}\n\n")

                # Performance por tema
                f.write("### ğŸ“š Rendimiento por Tema\n\n")
                for topic, stats in self.evaluation_results['topic_performance'].items():
                    success_rate = stats['success'] / stats['total'] if stats['total'] > 0 else 0
                    f.write(f"- **{topic.title()}:** {stats['success']}/{stats['total']} ({success_rate:.1%})\n")

                f.write("\n### ğŸšï¸ Rendimiento por Dificultad\n\n")
                for difficulty, stats in self.evaluation_results['difficulty_performance'].items():
                    success_rate = stats['success'] / stats['total'] if stats['total'] > 0 else 0
                    f.write(f"- **{difficulty.title()}:** {stats['success']}/{stats['total']} ({success_rate:.1%})\n")

                f.write("\n")

            # EstadÃ­sticas del modelo
            model_stats = self.model.get_model_stats()
            f.write("## ğŸ¤– EstadÃ­sticas del Modelo\n\n")
            f.write(f"- **Ejercicios generados:** {model_stats['generation_stats']['total_generated']}\n")
            f.write(f"- **Validaciones exitosas:** {model_stats['generation_stats']['successful_validations']}\n")
            f.write(
                f"- **Red neuronal entrenada:** {'âœ… SÃ­' if model_stats['neural_status']['is_trained'] else 'âŒ No'}\n")
            f.write(f"- **TamaÃ±o de vocabulario:** {model_stats['neural_status']['vocab_size']}\n\n")

            # Conclusiones
            f.write("## ğŸ“ Conclusiones\n\n")
            if self.evaluation_results:
                success_rate = self.evaluation_results['validation_success_rate']
                if success_rate > 0.8:
                    f.write("âœ… **Excelente:** El modelo muestra un rendimiento muy bueno.\n")
                elif success_rate > 0.6:
                    f.write("ğŸŸ¡ **Bueno:** El modelo funciona bien pero puede mejorarse.\n")
                else:
                    f.write("ğŸ”´ **Necesita mejoras:** El modelo requiere mÃ¡s entrenamiento.\n")

            f.write("\n## ğŸ¯ Recomendaciones\n\n")
            f.write("1. **Continuar entrenamiento** con mÃ¡s datos si la tasa de Ã©xito es baja\n")
            f.write("2. **Ajustar hiperparÃ¡metros** (learning rate, batch size) para optimizar\n")
            f.write("3. **Expandir dataset** con mÃ¡s variedad de ejercicios\n")
            f.write("4. **Integrar al proyecto** MATH_M1M para pruebas en producciÃ³n\n")

        print(f"ğŸ“„ Reporte generado: {report_filename}")
        return report_filename


def verify_system_requirements():
    """
    Verifica que todos los requisitos del sistema estÃ©n listos
    """
    print("ğŸ” VERIFICANDO REQUISITOS DEL SISTEMA")
    print("-" * 40)

    requirements_met = True

    # Verificar Python
    import sys
    python_version = sys.version_info
    print(f"ğŸ Python: {python_version.major}.{python_version.minor}.{python_version.micro}")
    if python_version < (3, 7):
        print("âŒ Se requiere Python 3.7 o superior")
        requirements_met = False
    else:
        print("âœ… VersiÃ³n de Python OK")

    # Verificar TensorFlow
    try:
        print(f"ğŸ§  TensorFlow: {tf.__version__}")

        # Verificar GPU (opcional)
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            print(f"ğŸš€ GPU disponible: {len(gpus)} dispositivo(s)")
        else:
            print("ğŸ’» Usando CPU (mÃ¡s lento pero funcional)")

        print("âœ… TensorFlow OK")
    except Exception as e:
        print(f"âŒ TensorFlow error: {e}")
        requirements_met = False

    # Verificar NumPy
    try:
        import numpy as np
        print(f"ğŸ”¢ NumPy: {np.__version__}")
        print("âœ… NumPy OK")
    except ImportError:
        print("âŒ NumPy no estÃ¡ disponible")
        requirements_met = False

    # Verificar espacio en disco
    import shutil
    try:
        free_space_gb = shutil.disk_usage('.').free / (1024 ** 3)
        print(f"ğŸ’¾ Espacio libre: {free_space_gb:.1f} GB")
        if free_space_gb < 1:
            print("âš ï¸ Poco espacio en disco - se recomienda al menos 1GB")
        else:
            print("âœ… Espacio en disco OK")
    except Exception:
        print("ğŸ’¾ No se pudo verificar espacio en disco")

    # Verificar memoria (estimaciÃ³n)
    try:
        import psutil
        memory_gb = psutil.virtual_memory().total / (1024 ** 3)
        print(f"ğŸ§  RAM total: {memory_gb:.1f} GB")
        if memory_gb < 4:
            print("âš ï¸ Poca RAM - el entrenamiento puede ser lento")
        else:
            print("âœ… RAM suficiente")
    except ImportError:
        print("ğŸ’¡ Para verificar RAM instala: pip install psutil")
    except Exception:
        print("ğŸ§  No se pudo verificar RAM")

    print("-" * 40)
    if requirements_met:
        print("âœ… Todos los requisitos crÃ­ticos estÃ¡n listos")
    else:
        print("âŒ Algunos requisitos crÃ­ticos no se cumplen")
        print("ğŸ’¡ Instala las dependencias faltantes antes de continuar")

    return requirements_met


def quick_model_test():
    """
    Hace una prueba rÃ¡pida del modelo antes del entrenamiento completo
    """
    print("\nğŸ§ª PRUEBA RÃPIDA DEL MODELO")
    print("-" * 30)

    try:
        # Crear modelo
        model = HybridMathModel()
        print("âœ… Modelo hÃ­brido creado")

        # Probar generaciÃ³n bÃ¡sica (sin entrenar)
        user_profile = {'edad': 12, 'rendimiento': 0.5, 'preferencia_visual': 0.5}
        exercise = model.generate_validated_exercise('aritmetica', 'basico', user_profile)

        print("âœ… GeneraciÃ³n bÃ¡sica funciona")
        print(f"   Problema: {exercise['problem']}")
        print(f"   Respuesta: {exercise['answer']}")

        # Verificar sistema experto
        validation = exercise.get('validation', {})
        if validation.get('is_valid'):
            print("âœ… Sistema experto funciona")
        else:
            print("âš ï¸ Sistema experto encontrÃ³ errores")

        return True

    except Exception as e:
        print(f"âŒ Error en prueba: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_trained_model(model: HybridMathModel):
    """
    Prueba interactiva del modelo entrenado
    """
    print(f"\nğŸ§ª PRUEBA INTERACTIVA DEL MODELO")
    print("-" * 40)

    usuario_prueba = {
        'edad': 12,
        'rendimiento': 0.7,
        'preferencia_visual': 0.5
    }

    while True:
        print(f"\nTemas disponibles: aritmetica, algebra, geometria, fracciones")
        tema = input("Elige un tema (o 'salir'): ").strip().lower()

        if tema == 'salir':
            break

        if tema not in ['aritmetica', 'algebra', 'geometria', 'fracciones']:
            print("âŒ Tema no vÃ¡lido")
            continue

        print("Dificultades: basico, intermedio, avanzado")
        dificultad = input("Elige dificultad: ").strip().lower()

        if dificultad not in ['basico', 'intermedio', 'avanzado']:
            print("âŒ Dificultad no vÃ¡lida")
            continue

        print(f"\nğŸ² Generando ejercicio de {tema} - {dificultad}...")

        try:
            ejercicio = model.generate_validated_exercise(tema, dificultad, usuario_prueba)

            print(f"\nğŸ“ EJERCICIO GENERADO:")
            print(f"ğŸ”¢ {ejercicio['problem']}")
            print(f"âœ… Respuesta: {ejercicio['answer']}")

            # Mostrar pasos si estÃ¡n disponibles
            if 'steps' in ejercicio and ejercicio['steps']:
                print(f"ğŸ“‹ Pasos de soluciÃ³n:")
                for i, step in enumerate(ejercicio['steps'], 1):
                    print(f"   {i}. {step}")

            validation = ejercicio.get('validation', {})
            print(f"ğŸ” VÃ¡lido: {'âœ…' if validation.get('is_valid') else 'âŒ'}")
            print(f"ğŸ“Š Confianza: {validation.get('confidence', 0):.2f}")
            print(f"ğŸ¯ Intentos: {validation.get('attempts_needed', 'N/A')}")

        except Exception as e:
            print(f"âŒ Error generando ejercicio: {e}")


def main():
    """
    FunciÃ³n principal para entrenar el modelo
    """
    print("ğŸ“ ENTRENAMIENTO DEL MODELO HÃBRIDO MATH_M1M")
    print("=" * 60)

    # Crear entrenador
    trainer = ModelTrainer()

    # ParÃ¡metros de entrenamiento
    print("\nâš™ï¸ ConfiguraciÃ³n del entrenamiento:")
    print("1. Â¿CuÃ¡ntas Ã©pocas de entrenamiento? (recomendado: 20-50)")
    try:
        epochs = int(input("Ã‰pocas: ") or "30")
        if epochs < 1:
            epochs = 30
    except ValueError:
        epochs = 30
        print(f"Valor invÃ¡lido, usando por defecto: {epochs}")

    print("2. Â¿TamaÃ±o del batch? (recomendado: 16-64)")
    try:
        batch_size = int(input("Batch size: ") or "32")
        if batch_size < 1:
            batch_size = 32
    except ValueError:
        batch_size = 32
        print(f"Valor invÃ¡lido, usando por defecto: {batch_size}")

    print("3. Â¿Usar dataset existente? (archivo .json)")
    dataset_file = input("Archivo dataset (Enter para generar nuevo): ").strip()
    if not dataset_file:
        dataset_file = None
    elif not os.path.exists(dataset_file):
        print(f"âš ï¸ Archivo no encontrado: {dataset_file}")
        print("Se generarÃ¡ un nuevo dataset")
        dataset_file = None

    print(f"\nğŸš€ ConfiguraciÃ³n final:")
    print(f"   Ã‰pocas: {epochs}")
    print(f"   Batch size: {batch_size}")
    print(f"   Dataset: {'Nuevo' if not dataset_file else dataset_file}")

    input("\nPresiona ENTER para continuar...")

    # Entrenar modelo
    print(f"\nğŸ¯ Iniciando entrenamiento...")
    history = trainer.train_model(
        dataset_file=dataset_file,
        epochs=epochs,
        batch_size=batch_size
    )

    if history:
        print(f"\nâœ… ENTRENAMIENTO COMPLETADO!")

        # Generar visualizaciones
        print(f"\nğŸ“Š Generando grÃ¡ficas...")
        try:
            trainer.plot_training_history()
        except Exception as e:
            print(f"âš ï¸ Error generando grÃ¡ficas: {e}")

        # Guardar modelo
        print(f"\nğŸ’¾ Guardando modelo entrenado...")
        try:
            model_file, training_file = trainer.save_trained_model()
        except Exception as e:
            print(f"âŒ Error guardando modelo: {e}")
            model_file, training_file = None, None

        # Generar reporte
        print(f"\nğŸ“„ Generando reporte...")
        try:
            report_file = trainer.generate_training_report()
        except Exception as e:
            print(f"âŒ Error generando reporte: {e}")
            report_file = None

        print(f"\nğŸ‰ Â¡PROCESO COMPLETADO!")
        print(f"ğŸ“ Archivos generados:")
        if model_file:
            print(f"   ğŸ¤– Modelo: {model_file}")
        if training_file:
            print(f"   ğŸ“Š Datos: {training_file}")
        if report_file:
            print(f"   ğŸ“„ Reporte: {report_file}")

        # Mostrar estadÃ­sticas finales
        if trainer.evaluation_results:
            print(f"\nğŸ“ˆ RESUMEN DE RENDIMIENTO:")
            success_rate = trainer.evaluation_results['validation_success_rate']
            print(f"   ğŸ“Š Tasa de Ã©xito: {success_rate:.1%}")
            print(f"   ğŸ¯ Confianza promedio: {trainer.evaluation_results['average_confidence']:.2f}")

            if success_rate > 0.8:
                print("   ğŸ‰ Â¡Excelente rendimiento!")
            elif success_rate > 0.6:
                print("   ğŸ‘ Buen rendimiento")
            else:
                print("   ğŸ“š Necesita mÃ¡s entrenamiento")

        # Probar modelo entrenado
        print(f"\nğŸ§ª Â¿Quieres probar el modelo entrenado? (s/n): ", end="")
        if input().lower() in ['s', 'si', 'sÃ­', 'y', 'yes']:
            test_trained_model(trainer.model)

    else:
        print(f"\nâŒ Error en el entrenamiento")
        print("ğŸ’¡ Verifica:")
        print("   - Que todos los archivos estÃ©n en la misma carpeta")
        print("   - Que TensorFlow estÃ© instalado correctamente")
        print("   - Que tengas suficiente memoria disponible")


if __name__ == "__main__":
    print("ğŸš€ INICIANDO SISTEMA DE ENTRENAMIENTO")
    print("=" * 50)

    # Verificar requisitos primero
    print("Paso 1: Verificando requisitos del sistema...")
    if not verify_system_requirements():
        print("\nâŒ Por favor resuelve los problemas de requisitos antes de continuar")
        print("ğŸ’¡ Dependencias mÃ­nimas:")
        print("   pip install tensorflow numpy")
        print("   pip install matplotlib  # (opcional, para grÃ¡ficas)")
        exit(1)

    # Prueba rÃ¡pida
    print("\nPaso 2: Probando el modelo...")
    if not quick_model_test():
        print("\nâŒ Prueba del modelo fallÃ³. Verifica los archivos.")
        print("ğŸ’¡ AsegÃºrate de tener:")
        print("   - hybrid_model.py")
        print("   - training_data_generator.py")
        print("   - training_script.py")
        exit(1)

    print("\nâœ… Todas las verificaciones pasaron!")
    print("ğŸ“ Procediendo con el entrenamiento...\n")

    # Proceder con entrenamiento
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Entrenamiento interrumpido por el usuario")
        print("ğŸ’¾ Los datos parciales pueden haberse guardado")
    except Exception as e:
        print(f"\nâŒ Error inesperado: {e}")
        import traceback

        traceback.print_exc()
        print("\nğŸ’¡ Si el error persiste, verifica:")
        print("   - Versiones de las dependencias")
        print("   - Espacio disponible en disco")
        print("   - Memoria RAM disponible")